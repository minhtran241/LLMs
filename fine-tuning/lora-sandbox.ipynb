{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Sandbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/.pyenv/versions/3.11.0/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How dataset was generated\n",
    "# # Load the imdb dataset\n",
    "# imdb_dataset = load_dataset(\"imdb\")\n",
    "# # Subsample size (this is the size of the dataset used for training and testing)\n",
    "# N = 1000\n",
    "# # Generate indexes for random subsample (we don't want to use the entire dataset for training and testing, as it would take too long to train and test the model)\n",
    "# rand_idx = np.random.randint(24999, size=N) # array of N random indexes\n",
    "# # Extract train and test data\n",
    "# x_train = imdb_dataset['train'][rand_idx]['text']\n",
    "# y_train = imdb_dataset['train'][rand_idx]['label']\n",
    "\n",
    "# x_test = imdb_dataset['test'][rand_idx]['text']\n",
    "# y_test = imdb_dataset['test'][rand_idx]['label']\n",
    "\n",
    "# # Create new dataset\n",
    "# train_dataset = DatasetDict({\n",
    "# \t'train': Dataset.from_dict({'text': x_train, 'label': y_train}),\n",
    "# \t'test': Dataset.from_dict({'text': x_test, 'label': y_test})\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dataset = load_dataset(\"shawhin/imdb-truncated\")\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display % of training data with label 1 (positive)\n",
    "np.array(train_dataset[\"train\"][\"label\"]).sum() / len(train_dataset[\"train\"][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = (\n",
    "    \"distilbert-base-uncased\"  # We use a smaller model for faster training\n",
    ")\n",
    "\n",
    "# Define label maps\n",
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "# Generate classification model from model checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"negative\",\n",
       "    \"1\": \"positive\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"negative\": 0,\n",
       "    \"positive\": 1\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display architecture of the model\n",
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint, add_prefix_space=True\n",
    ")  # add_prefix_space=True is used to add a space before the first token to avoid a warning. The warning is due to the fact that the model was trained with a space before the first token, but the tokenizer does not add it by default.\n",
    "\n",
    "# Add pad token to the tokenizer if it does not exist. The pad token is used to pad sequences to the same length.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # Extract text from examples\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    # Tokenize and truncate text\n",
    "    tokenizer.truncate_side = \"left\"  # truncate from the left side because the model is trained to read from left to right\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text, return_tensors=\"np\", truncation=True, max_length=512\n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize training and validation datasets\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator: This is used to pad sequences to the same length. The model requires sequences to be of the same length.\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an evaluation function that will be called at the end of each epoch. Epoch is a complete pass through the entire training dataset. For example, if the training dataset has 1000 samples and the batch size is 10, then there are 100 batches in an epoch.\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # Extract predictions and labels\n",
    "    predictions, labels = (\n",
    "        p  # predictions are the model's output. For sequence classification, the output is the logits (scores) for each class. The class with the highest score is the predicted class. Example: [[-1.2, 1.5], [0.9, -2.1]] means the first sample is predicted as class 1 and the second sample is predicted as class 0. labels are the true labels. Example: [1, 0] means the first sample is class 1 and the second sample is class 0.\n",
    "    )\n",
    "\n",
    "    # Convert logits to predicted class\n",
    "    predictions = np.argmax(\n",
    "        predictions, axis=1\n",
    "    )  # convert logits to predicted class. Example: [[-1.2, 1.5], [0.9, -2.1]] becomes [1, 0]. [-1.2, 1.5] and [0.9, -2.1] are the logits for the first and second samples, respectively.\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply untrained model to text data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "Text: This is a great movie!\n",
      "Predicted label: negative\n",
      "Logits: tensor([ 0.0271, -0.1207], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Text: This is a great movie!\n",
      "Predicted label: negative\n",
      "Logits: tensor([ 0.0248, -0.1351], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Text: This is a great movie!\n",
      "Predicted label: negative\n",
      "Logits: tensor([ 0.0656, -0.0649], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Text: This is a great movie!\n",
      "Predicted label: negative\n",
      "Logits: tensor([ 0.0603, -0.0611], grad_fn=<SelectBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define list of text examples\n",
    "text_list = [\n",
    "    \"This is a great movie!\",\n",
    "    \"This is a bad movie!\",\n",
    "    \"This movie is not good.\",\n",
    "    \"This movie is not bad.\",\n",
    "]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer.encode(\n",
    "        text, return_tensors=\"pt\"\n",
    "    )  # return_tensors=\"pt\" is used to return PyTorch tensors\n",
    "    # Compute logits (Logits are the scores for each class. The class with the highest score is the predicted class)\n",
    "    logits = model(inputs).logits\n",
    "    # Convert logits to label\n",
    "    predictions = torch.argmax(logits, dim=1)\n",
    "    # Convert list of tensors to list of integers\n",
    "    predictions = predictions.tolist()\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        print(f\"Text: {text_list[i]}\")\n",
    "        print(f\"Predicted label: {id2label[prediction]}\")\n",
    "        print(f\"Logits: {logits[i]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # task type. In this case, it is sequence classification\n",
    "    r=4,  # number of layers in the PEFT model\n",
    "    lora_alpha=32,  # alpha parameter for LoRA. LoRA is used to control the flow of information between layers in the PEFT model. Alpha is a hyperparameter that controls the strength of the connections between layers. A higher alpha means stronger connections between layers.\n",
    "    lora_dropout=0.01,  # dropout rate for LoRA. Dropout is used to prevent overfitting. It randomly sets a fraction of the input units to 0 at each update during training time, which helps prevent overfitting.\n",
    "    target_modules=[\n",
    "        \"q_lin\"\n",
    "    ],  # The target modules are the modules that are controlled by LoRA. In this case, we are controlling the linear layers (q_lin) in the PEFT model.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, inference_mode=False, r=4, target_modules={'q_lin'}, lora_alpha=32, lora_dropout=0.01, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 628,994 || all params: 67,584,004 || trainable%: 0.9306847223789819\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 1e-3  # This is the learning rate. It controls the step size during training. A higher learning rate means larger steps and faster training, but it can also lead to divergence. A lower learning rate means smaller steps and slower training, but it can also lead to better convergence.\n",
    "batch_size = 4  # The number of samples in a batch. A batch is used to update the model's weights. A smaller batch size means the model is updated more frequently, but it also means the training process is slower. A larger batch size means the model is updated less frequently, but it also means the training process is faster. What inside batch are model inputs and labels.\n",
    "\n",
    "num_epochs = 10  # The number of times the entire training dataset is passed through the model. Each pass is called an epoch. A higher number of epochs means the model is trained for a longer time, but it can also lead to overfitting. A lower number of epochs means the model is trained for a shorter time, but it can also lead to underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_checkpoint\n",
    "    + \"-lora-text-classification\",  # The output directory where the model predictions and checkpoints will be written\n",
    "    learning_rate=lr,  # The initial learning rate for training the model\n",
    "    per_device_train_batch_size=batch_size,  # The batch size for training. The number of samples in a batch used to update the model's weights during training.\n",
    "    per_device_eval_batch_size=batch_size,  # The batch size for evaluation. The number of samples in a batch used to evaluate the model during training.\n",
    "    num_train_epochs=num_epochs,  # The number of epochs for training the model (the number of times the entire training dataset is passed through the model)\n",
    "    weight_decay=0.01,  # Weight decay is a regularization technique. It adds a penalty term to the loss function to prevent overfitting. A higher weight decay means a stronger penalty, which means the model is more regularized.\n",
    "    evaluation_strategy=\"epoch\",  # The evaluation strategy to use at the end of each epoch. In this case, we evaluate the model at the end of each epoch.\n",
    "    save_strategy=\"epoch\",  # The strategy to save the model. In this case, we save the model at the end of each epoch.\n",
    "    load_best_model_at_end=True,  # Whether or not to load the best model at the end of training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 250/2500 [04:07<14:57,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.55406653881073, 'eval_accuracy': {'accuracy': 0.83}, 'eval_runtime': 102.3622, 'eval_samples_per_second': 9.769, 'eval_steps_per_second': 2.442, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 500/2500 [06:34<15:07,  2.20it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4226, 'grad_norm': 2.205007791519165, 'learning_rate': 0.0008, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 500/2500 [06:59<15:07,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44499048590660095, 'eval_accuracy': {'accuracy': 0.87}, 'eval_runtime': 24.8647, 'eval_samples_per_second': 40.218, 'eval_steps_per_second': 10.054, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 30%|███       | 750/2500 [09:15<13:23,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7752700448036194, 'eval_accuracy': {'accuracy': 0.862}, 'eval_runtime': 23.8076, 'eval_samples_per_second': 42.003, 'eval_steps_per_second': 10.501, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1000/2500 [10:56<06:50,  3.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1821, 'grad_norm': 0.060890164226293564, 'learning_rate': 0.0006, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 1000/2500 [11:18<06:50,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6749710440635681, 'eval_accuracy': {'accuracy': 0.894}, 'eval_runtime': 22.1852, 'eval_samples_per_second': 45.075, 'eval_steps_per_second': 11.269, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 50%|█████     | 1250/2500 [13:02<05:19,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.810627818107605, 'eval_accuracy': {'accuracy': 0.895}, 'eval_runtime': 21.9733, 'eval_samples_per_second': 45.51, 'eval_steps_per_second': 11.377, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1500/2500 [14:24<03:20,  4.99it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0431, 'grad_norm': 0.013209199532866478, 'learning_rate': 0.0004, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 1500/2500 [14:49<03:20,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.8653650879859924, 'eval_accuracy': {'accuracy': 0.895}, 'eval_runtime': 24.9744, 'eval_samples_per_second': 40.041, 'eval_steps_per_second': 10.01, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 70%|███████   | 1750/2500 [16:33<02:56,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.996708869934082, 'eval_accuracy': {'accuracy': 0.886}, 'eval_runtime': 27.1848, 'eval_samples_per_second': 36.785, 'eval_steps_per_second': 9.196, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 2000/2500 [17:45<01:49,  4.55it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.013, 'grad_norm': 0.1288709193468094, 'learning_rate': 0.0002, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 2000/2500 [18:08<01:49,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9884560108184814, 'eval_accuracy': {'accuracy': 0.89}, 'eval_runtime': 23.4856, 'eval_samples_per_second': 42.579, 'eval_steps_per_second': 10.645, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 90%|█████████ | 2250/2500 [19:52<01:08,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9701987504959106, 'eval_accuracy': {'accuracy': 0.887}, 'eval_runtime': 24.0707, 'eval_samples_per_second': 41.544, 'eval_steps_per_second': 10.386, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [20:57<00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0102, 'grad_norm': 4.3668656871886924e-05, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 2500/2500 [21:21<00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9624693989753723, 'eval_accuracy': {'accuracy': 0.892}, 'eval_runtime': 23.2819, 'eval_samples_per_second': 42.952, 'eval_steps_per_second': 10.738, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [21:21<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1281.6845, 'train_samples_per_second': 7.802, 'train_steps_per_second': 1.951, 'train_loss': 0.13418660144805908, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=0.13418660144805908, metrics={'train_runtime': 1281.6845, 'train_samples_per_second': 7.802, 'train_steps_per_second': 1.951, 'train_loss': 0.13418660144805908, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,  # The model to train\n",
    "    args=training_args,  # The training arguments\n",
    "    train_dataset=tokenized_dataset[\"train\"],  # The training dataset\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],  # The evaluation dataset\n",
    "    tokenizer=tokenizer,  # The tokenizer used to tokenize the data\n",
    "    data_collator=data_collator,  # The data collator used to pad sequences to the same length\n",
    "    compute_metrics=compute_metrics,  # The function used to compute metrics at the end of each epoch\n",
    ")\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "This is a great movie! - positive\n",
      "This is a bad movie! - negative\n",
      "This movie is not good. - negative\n",
      "This movie is not bad. - negative\n"
     ]
    }
   ],
   "source": [
    "model.to('mps') # moving to mps for Mac (can alternatively do 'cpu')\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\") # moving to mps for Mac (can alternatively do 'cpu')\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
